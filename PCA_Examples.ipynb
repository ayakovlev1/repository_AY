{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dabfa2e6",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f4c93",
   "metadata": {},
   "source": [
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n",
    "\n",
    "In this notebook, we will walk through a couple of examples demonstrating PCA using the Python library `scikit-learn`.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a4fceb",
   "metadata": {},
   "source": [
    "## Example 1: Basic PCA on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75e77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generating synthetic data\n",
    "np.random.seed(0)\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0.8], [0.8, 1]]  # diagonal covariance\n",
    "X = np.random.multivariate_normal(mean, cov, 100)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plotting the original and PCA-transformed data\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.7)\n",
    "plt.title('Original Data')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7)\n",
    "plt.title('PCA-transformed Data')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2886d89",
   "metadata": {},
   "source": [
    "## Example 2: PCA on the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758524ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Standardizing the data\n",
    "X_iris_scaled = StandardScaler().fit_transform(X_iris)\n",
    "\n",
    "# Applying PCA\n",
    "pca_iris = PCA(n_components=2)\n",
    "X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n",
    "\n",
    "# Plotting the PCA-transformed Iris dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(X_iris_pca[:, 0], X_iris_pca[:, 1], hue=iris.target_names[y_iris], palette='Set1', s=60, alpha=0.8)\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
